
# coding: utf-8

# In[ ]:

# **前言**
'''
      ----参考华泰证券《人工智能选股之 Boosting 模型》研报
最近，人工智能引起了大家广泛的关注，其在图像识别，自然语言处理方向都做出了一些成果。该领域比较常用的模型有线性回归、树模型、SVM， 集成学习，深度学习模型(CNN, RNN)，以往大家在量化方面主要选取线性回归模型，其在解释因子收益方面比较直观，但这种做法会丧失一些非线性的特征关系。
本文主要考察集成学习在量化选股方面的运用，同上述研报一样，主要采用集成学习中的boosting方法， 选取了xgboost作为训练框架，对选取的因子进行合成，最终考察该合成因子的选股效果。
'''
# In[ ]:

# **数据准备**
'''

--------
本文选取了优矿的70个因子，提取股票每个月末的因子暴露作为训练输入特征。读者也可以选取自己感兴趣的因子作为基础因子。
特征按照研报中所描述做中位数去极值，缺失值处理，标准化等，该处理后的数据作为模型输入特征。同时在每个月末截面期，选取下月收益排名前30%的股票作为正例（𝑦=1），后30%的股票作为负例（𝑦=−1）。利用xgboost进行分类预测。由于处理数据源代码过长，这里不做展示，且该部分也不具备参考价值，读者可根据自己常用特征处理习惯进行处理。
'''

# In[ ]:

import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt

factors = [b'Beta60', b'OperatingRevenueGrowRate', b'NetProfitGrowRate', b'NetCashFlowGrowRate', b'NetProfitGrowRate5Y', b'TVSTD20',
           b'TVSTD6', b'TVMA20', b'TVMA6', b'BLEV', b'MLEV', b'CashToCurrentLiability', b'CurrentRatio', b'REC', b'DAREC', b'GREC',
           b'DASREV', b'SFY12P', b'LCAP', b'ASSI', b'LFLO', b'TA2EV', b'PEG5Y', b'PE', b'PB', b'PS', b'SalesCostRatio', b'PCF', b'CETOP',
           b'TotalProfitGrowRate', b'CTOP', b'MACD', b'DEA', b'DIFF', b'RSI', b'PSY', b'BIAS10', b'ROE', b'ROA', b'ROA5', b'ROE5',
           b'DEGM', b'GrossIncomeRatio', b'ROECut', b'NIAPCut', b'CurrentAssetsTRate', b'FixedAssetsTRate', b'FCFF', b'FCFE', b'PLRC6',
           b'REVS5', b'REVS10', b'REVS20', b'REVS60', b'HSIGMA', b'HsigmaCNE5', b'ChaikinOscillator', b'ChaikinVolatility', b'Aroon',
           b'DDI', b'MTM', b'MTMMA', b'VOL10', b'VOL20', b'VOL5', b'VOL60', b'RealizedVolatility', b'DASTD', b'DDNSR', b'Hurst']

df = pd.read_csv(u'./raw_data/dataset.csv', dtype={"ticker": np.str, "tradeDate": np.str, "next_month_end": np.str},index_col=0, encoding='GBK')
df.head()


# # **模型训练**
# -----
# 为了能让模型及时抓取到市场的变化，我们采用了七个阶段滚动回测方法。模型训练区间为20070101至20171231，按年份分为7个子区间，因此需要对每个子回测的不同训练集重复训练。
# 每次训练完模型的测试必须选用训练样本外的数据，一般测试样本选取一年的数据，训练样本选取不超过6年的历史数据，因为优矿只提供07年后的数据，所以初始的几个样本不足7年。比如第一阶段选取07-10的数据为训练样本，11年为测试样本；第二阶段选取07-11的数据作为训练样本，12年为测试样本…以此内推，最后一个阶段为11-16的数据为训练样本，17年为测试样本。
# 另外模型在训练中，输入数据按照90%与10%的比例拆成训练集与验证集。训练中，保存在验证集上效果最好的模型。模型中参数可调，读者可更换一些参数，查看结果变更。

# In[ ]:

class BoostModel:
    def __init__(self, max_depth=3, subsample=0.95, num_round=2000, early_stopping_rounds=50):
        self.params = {'max_depth': max_depth, 'eta': 0.1, 'silent': 1, 'alpha': 0.5, 'lambda': 0.5, 'eval_metric':'auc', 'subsample':subsample, 'objective': 'binary:logistic'}
        self.num_round = num_round
        self.early_stopping_rounds = early_stopping_rounds


    def fit(self, train_data, train_label,  val_data, val_label):
        dtrain = xgb.DMatrix(train_data, label=train_label)
        deval = xgb.DMatrix(val_data, label=val_label)

        boost_model = xgb.train(self.params, dtrain, num_boost_round=self.num_round, evals=[(dtrain,'train'), (deval, 'eval')], early_stopping_rounds=self.early_stopping_rounds, verbose_eval=False)
        print('get best eval auc : %s, in step %s'%(boost_model.best_score, boost_model.best_iteration))
        self.boost_model = boost_model
        
        return boost_model


    def predict(self, test_data):
        dtest = xgb.DMatrix(test_data)
        predict_score = self.boost_model.predict(dtest, ntree_limit=self.boost_model.best_ntree_limit)
        
        return predict_score
        


# In[ ]:

def get_train_val_test_data(year, split_pct=0.9):
    back_year = max(2007, year-6)
    train_val_df = df[(df['year']>=back_year) & (df['year']<year)]
    train_val_df = train_val_df.sample(frac=1).reset_index(drop=True)
    
    #拆分训练集、验证集
    train_df = train_val_df.iloc[0:int(len(train_val_df) * split_pct)]
    val_df = train_val_df.iloc[int(len(train_val_df) * split_pct):]
    
    test_df = df[df['year']==year]
    
    return train_df, val_df, test_df

def format_feature_label(origin_df, is_filter=True):
    
    if is_filter:
        origin_df = origin_df[origin_df['label']!=0]
        #因子xgboost的label输入范围只能是[0, 1]，需要对原始label进行替换
        origin_df['label'] = origin_df['label'].replace(-1, 0)
        
    feature = np.array(origin_df[factors])
    label = np.array(origin_df['label'])

    return feature, label

def write_factor_to_csv(df, predict_score, year):
    #记录模型预测分数为因子值，输出
    df['factor'] = predict_score
    df = df.loc[:, ['ticker', 'tradeDate', 'label', 'factor']]
    is_header = True
    if year != 2011:
        is_header = False
    
    df.to_csv('./raw_data/factor.csv', mode='a+', encoding='utf-8', header=is_header)

def pipeline():
    boost_model_list = []
    for year in range(2011, 2018):
        print('training model for %s' % year)
        train_df, val_df, test_df = get_train_val_test_data(year)
        boost_model = BoostModel()
        train_feature, train_label = format_feature_label(train_df)
        val_feature, val_label = format_feature_label(val_df)
        
        boost_model.fit(train_feature, train_label, val_feature, val_label)
        
        test_feature, test_label = format_feature_label(test_df, False)
        predict_score = boost_model.predict(test_feature)
        
        write_factor_to_csv(test_df, predict_score, year)
        boost_model_list.append(boost_model)
    
    return boost_model_list

boost_model_list = pipeline()


# # **模型结果分析**
# ------
# 上述信息只展示了模型验证集上的效果，现在让我们来查看一下样本外的准确率如何。可以看到7个阶段的平均准确率在57%左右，评价AUC在60%左右。

# In[ ]:

from datetime import datetime
from sklearn.metrics import roc_auc_score

#计算二分类模型样本外的ACC与AUC
def get_test_auc_acc():
    df = pd.read_csv('./raw_data/factor.csv')
    #只查看原有label为+1, -1的数据
    df = df[df['label'] != 0]
    df.loc[:, 'predict'] = df.loc[:, 'factor'].apply(lambda x : 1 if x > 0.5 else -1)

    acc_list = []
    auc_list = []
    for date, group in df.groupby('tradeDate'):
        df_correct = group[group['predict'] == group['label']]
        correct = len(df_correct) * 1.0 / len(group)
        auc =  roc_auc_score(np.array(group['label']), np.array(group['factor']))
        acc_list.append([date, correct])
        auc_list.append([date, auc])
        
    acc_list = sorted(acc_list, key=lambda x: x[0], reverse=False)
    mean_acc = sum([item[1] for item in acc_list]) / len(acc_list)
    
    auc_list = sorted(auc_list, key=lambda x: x[0], reverse=False)
    mean_auc = sum([item[1] for item in auc_list]) / len(auc_list)
    
    return acc_list, auc_list, round(mean_acc, 2), round(mean_auc, 2)

def plot_accuracy_curve():
    acc_list, auc_list, mean_acc, mean_auc = get_test_auc_acc()

    plt.plot([datetime.strptime(str(item[0]), '%Y%m%d') for item in acc_list], [item[1] for item in acc_list], '-bo')
    plt.plot([datetime.strptime(str(item[0]), '%Y%m%d') for item in auc_list], [item[1] for item in auc_list], '-ro')

    plt.legend([u"acc curve: mean_acc:%s"%mean_acc, u"auc curve: mean auc:%s"%mean_auc], loc='upper left', handlelength=2, handletextpad=0.5, borderpad=0.1)
    plt.ylim((0.3, 0.8))
    plt.show()

plot_accuracy_curve()


# 同时，我们也可以查看特征的重要性，可以直观感受哪些因子的影响比较重要。首先统计各个年份的特征排序，排序范围1~70，1为影响最弱，70为影响最强。最后求出均值。
# 以下表格列出了排名前10和排名后10的因子名称，查看优矿因子文档，知道Aroon(动量因子)，Hurst(赫斯特指数, 技术指标类因子), ChaikinOscillator(佳庆指标, 技术指标类因子), LCAP(对数市值), REVS20(动量类因子)在模型分类中表现最为重要。这里需要注意的是，很多因子之间本身有相关性，所以表现好坏并不能作为该因子的唯一判断标准。 

# In[ ]:

def get_feature_importance():
    df = pd.DataFrame(index=factors, columns=range(2011, 2018))
    for i, column in enumerate(range(2011, 2018)):
        feature_importance = boost_model_list[i].boost_model.get_score(importance_type='weight')
        df[column] = pd.Series(index=[factors[int(key.replace('f', ''))] for key, value in feature_importance.items()], data =[value for key, value in feature_importance.items()])
        df[column] = df[column].fillna(0.0)
        df[column] = 1 + np.argsort(np.argsort(df[column]))
        
    df['all'] = df.mean(axis=1)
    
    return df.sort_values('all', ascending=False)
        
feature_importance_df = get_feature_importance()
feature_importance_df.iloc[np.r_[0:10, -10:0]]


# # **因子回测**
# 上述模型在测试的时候，为每个股票打了一个分数，可以认为是将输入的70个因子转换成了一个集成的“因子”，现在让我们来检测这个非线性输出的因子回测效果如何。
# 本文选取了中证500作为基准，并分为5组查看每类效果。回测框架参考优矿[API文档](https://uqer.io/help/api/#quick_backtest%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96) 中quick_backtest参数优化，画图参考之前社区贴[凤鸣朝阳 - 股价日内模式分析](https://uqer.io/community/share/581aa67a228e5b43fa5c363f)

# In[ ]:

signal_df = pd.read_csv(u'./raw_data/factor.csv', dtype={"ticker": np.str, "tradeDate": np.str},index_col=0, encoding='GBK')
signal_df['ticker'] = signal_df['ticker'].apply(lambda x: x+'.XSHG' if x[:2] in ['60'] else x+'.XSHE')
signal_df = signal_df[[u'ticker', u'tradeDate', u'factor']]

# 申万行业
sw_frame = DataAPI.EquIndustryGet(industryVersionCD=u"010303",industry=u"",secID=u"",ticker=u"",intoDate=u"",field=u"",pandas="1")
sw_frame = sw_frame[sw_frame.isNew==1][['secID', 'ticker', 'industryName1', 'industryName2', 'industryName3']]
sw_frame = sw_frame[['secID', 'industryName1']]
sw_frame.columns = ['ticker', 'industryName1']
signal_df = signal_df.merge(sw_frame, on=['ticker'], how='left')
signal_df.head()


# In[ ]:

from CAL.PyCAL import * 

# -----------回测参数部分开始，可编辑------------
start = '2011-01-01'                       # 回测起始时间
end = '2017-12-31'                         # 回测结束时间
benchmark = 'ZZ500'                        # 策略参考标准
universe = DynamicUniverse('ZZ500')           # 证券池，支持股票和基金
capital_base = 10000000                     # 起始资金
freq = 'd'                              
refresh_rate = Monthly(1)  

factor_data = signal_df[['ticker', 'tradeDate', 'factor']]     # 读取因子数据
factor_data = factor_data.set_index('tradeDate', drop=True)
q_dates = factor_data.index.values

accounts = {
    'fantasy_account': AccountConfig(account_type='security', capital_base=10000000)
}

# ---------------回测参数部分结束----------------

# 把回测参数封装到 SimulationParameters 中，供 quick_backtest 使用
sim_params = quartz.SimulationParameters(start, end, benchmark, universe, capital_base, refresh_rate=refresh_rate, accounts=accounts)
# 获取回测行情数据
data = quartz.get_backtest_data(sim_params)
# 运行结果
results = {}

# 调整参数(选取股票的集成因子五分位数)，进行快速回测
for quantile_five in range(1, 6):
    
    # ---------------策略逻辑部分----------------
    
    def initialize(context):                   # 初始化虚拟账户状态
        pass

    def handle_data(context): 
        account = context.get_account('fantasy_account')
        current_universe = context.get_universe('stock', exclude_halt=True)
        pre_date = context.previous_date.strftime("%Y%m%d")
        if pre_date not in q_dates:            
            return

        # 拿取调仓日前一个交易日的因子，并按照相应十分位选择股票
        q = factor_data.ix[pre_date].dropna()
        q = q.set_index('ticker', drop=True)
        q = q.ix[current_universe]
        
        q_min = q['factor'].quantile((quantile_five-1)*0.2)
        q_max = q['factor'].quantile(quantile_five*0.2)
        my_univ = q[(q['factor']>=q_min) & (q['factor']<q_max)].index.values

       # 交易部分
        positions = account.get_positions()
        sell_list = [stk for stk in positions if stk not in my_univ]
        for stk in sell_list:
            order_to(stk,0)
        
        # 在目标股票池中的，等权买入
        for stk in my_univ:
            order_pct_to(stk, 1.0/len(my_univ))


    # 生成策略对象
    strategy = quartz.TradingStrategy(initialize, handle_data)
    # ---------------策略定义结束----------------
    
    # 开始回测
    bt, perf = quartz.quick_backtest(sim_params, strategy, data=data)

    # 保存运行结果，1为因子最强组，5为因子最弱组
    results[6 - quantile_five] = {'max_drawdown': perf['max_drawdown'], 'sharpe': perf['sharpe'], 'alpha': perf['alpha'], 'beta': perf['beta'],
                              'information_ratio': perf['information_ratio'], 'annualized_return': perf['annualized_return'], 'bt': bt}    

    print str(quantile_five),
print 'done'


# In[ ]:

import seaborn as sns
sns.set_style('white')

fig = plt.figure(figsize=(10,8))
fig.set_tight_layout(True)
ax1 = fig.add_subplot(211)
ax2 = fig.add_subplot(212)
ax1.grid()
ax2.grid()

for qt in results:
    bt = results[qt]['bt']

    data = bt[[u'tradeDate',u'portfolio_value',u'benchmark_return']]
    data['portfolio_return'] = data.portfolio_value/data.portfolio_value.shift(1) - 1.0   # 总头寸每日回报率
    data['portfolio_return'].ix[0] = data['portfolio_value'].ix[0]/	10000000.0 - 1.0
    data['excess_return'] = data.portfolio_return - data.benchmark_return                 # 总头寸每日超额回报率
    data['excess'] = data.excess_return + 1.0
    data['excess'] = data.excess.cumprod()                # 总头寸对冲指数后的净值序列
    data['portfolio'] = data.portfolio_return + 1.0     
    data['portfolio'] = data.portfolio.cumprod()          # 总头寸不对冲时的净值序列
    data['benchmark'] = data.benchmark_return + 1.0
    data['benchmark'] = data.benchmark.cumprod()          # benchmark的净值序列
    results[qt]['hedged_max_drawdown'] = max([1 - v/max(1, max(data['excess'][:i+1])) for i,v in enumerate(data['excess'])])  # 对冲后净值最大回撤
    results[qt]['hedged_volatility'] = np.std(data['excess_return'])*np.sqrt(252)
    results[qt]['hedged_annualized_return'] = (data['excess'].values[-1])**(252.0/len(data['excess'])) - 1.0
    ax1.plot(data['tradeDate'], data[['portfolio']], label=str(qt))
    ax2.plot(data['tradeDate'], data[['excess']], label=str(qt))
    

ax1.legend(loc=0)
ax2.legend(loc=0)
ax1.set_ylabel(u"净值", fontproperties=font, fontsize=16)
ax2.set_ylabel(u"对冲净值", fontproperties=font, fontsize=16)
ax1.set_title(u"因子不同五分位数分组选股净值走势", fontproperties=font, fontsize=16)
ax2.set_title(u"因子不同五分位数分组选股对冲中证500指数后净值走势", fontproperties=font, fontsize=16)

# results 转换为 DataFrame
results_pd = pd.DataFrame(results).T.sort_index()

results_pd = results_pd[[u'alpha', u'beta', u'information_ratio', u'sharpe', u'annualized_return', u'max_drawdown',  
                         u'hedged_annualized_return', u'hedged_max_drawdown', u'hedged_volatility']]

cols = [(u'风险指标', u'Alpha'), (u'风险指标', u'Beta'), (u'风险指标', u'信息比率'), (u'风险指标', u'夏普比率'), (u'纯股票多头时', u'年化收益'),
        (u'纯股票多头时', u'最大回撤'), (u'对冲后', u'年化收益'), (u'对冲后', u'最大回撤'), (u'对冲后', u'收益波动率')]
results_pd.columns = pd.MultiIndex.from_tuples(cols)
results_pd.index.name = u'五分位组别'
results_pd


# 可以从上图看到1-5组有明显单调关系，可以说明该因子有一定的选股能力。
# 上述一些输入仅作参考，读者可以根据自己需要更改模型输入基础因子，模型训练参数及框架，来进行因子回测。

# # **参考**
# 1、 [凤鸣朝阳 - 股价日内模式分析](https://uqer.io/community/share/581aa67a228e5b43fa5c363f)
# 2、 华泰证券 《人工智能选股之 Boosting 模型》
