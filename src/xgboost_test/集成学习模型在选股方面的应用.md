
# coding: utf-8

# In[ ]:

# **å‰è¨€**
'''
      ----å‚è€ƒåŽæ³°è¯åˆ¸ã€Šäººå·¥æ™ºèƒ½é€‰è‚¡ä¹‹ Boosting æ¨¡åž‹ã€‹ç ”æŠ¥
æœ€è¿‘ï¼Œäººå·¥æ™ºèƒ½å¼•èµ·äº†å¤§å®¶å¹¿æ³›çš„å…³æ³¨ï¼Œå…¶åœ¨å›¾åƒè¯†åˆ«ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†æ–¹å‘éƒ½åšå‡ºäº†ä¸€äº›æˆæžœã€‚è¯¥é¢†åŸŸæ¯”è¾ƒå¸¸ç”¨çš„æ¨¡åž‹æœ‰çº¿æ€§å›žå½’ã€æ ‘æ¨¡åž‹ã€SVMï¼Œ é›†æˆå­¦ä¹ ï¼Œæ·±åº¦å­¦ä¹ æ¨¡åž‹(CNN, RNN)ï¼Œä»¥å¾€å¤§å®¶åœ¨é‡åŒ–æ–¹é¢ä¸»è¦é€‰å–çº¿æ€§å›žå½’æ¨¡åž‹ï¼Œå…¶åœ¨è§£é‡Šå› å­æ”¶ç›Šæ–¹é¢æ¯”è¾ƒç›´è§‚ï¼Œä½†è¿™ç§åšæ³•ä¼šä¸§å¤±ä¸€äº›éžçº¿æ€§çš„ç‰¹å¾å…³ç³»ã€‚
æœ¬æ–‡ä¸»è¦è€ƒå¯Ÿé›†æˆå­¦ä¹ åœ¨é‡åŒ–é€‰è‚¡æ–¹é¢çš„è¿ç”¨ï¼ŒåŒä¸Šè¿°ç ”æŠ¥ä¸€æ ·ï¼Œä¸»è¦é‡‡ç”¨é›†æˆå­¦ä¹ ä¸­çš„boostingæ–¹æ³•ï¼Œ é€‰å–äº†xgboostä½œä¸ºè®­ç»ƒæ¡†æž¶ï¼Œå¯¹é€‰å–çš„å› å­è¿›è¡Œåˆæˆï¼Œæœ€ç»ˆè€ƒå¯Ÿè¯¥åˆæˆå› å­çš„é€‰è‚¡æ•ˆæžœã€‚
'''
# In[ ]:

# **æ•°æ®å‡†å¤‡**
'''

--------
æœ¬æ–‡é€‰å–äº†ä¼˜çŸ¿çš„70ä¸ªå› å­ï¼Œæå–è‚¡ç¥¨æ¯ä¸ªæœˆæœ«çš„å› å­æš´éœ²ä½œä¸ºè®­ç»ƒè¾“å…¥ç‰¹å¾ã€‚è¯»è€…ä¹Ÿå¯ä»¥é€‰å–è‡ªå·±æ„Ÿå…´è¶£çš„å› å­ä½œä¸ºåŸºç¡€å› å­ã€‚
ç‰¹å¾æŒ‰ç…§ç ”æŠ¥ä¸­æ‰€æè¿°åšä¸­ä½æ•°åŽ»æžå€¼ï¼Œç¼ºå¤±å€¼å¤„ç†ï¼Œæ ‡å‡†åŒ–ç­‰ï¼Œè¯¥å¤„ç†åŽçš„æ•°æ®ä½œä¸ºæ¨¡åž‹è¾“å…¥ç‰¹å¾ã€‚åŒæ—¶åœ¨æ¯ä¸ªæœˆæœ«æˆªé¢æœŸï¼Œé€‰å–ä¸‹æœˆæ”¶ç›ŠæŽ’åå‰30%çš„è‚¡ç¥¨ä½œä¸ºæ­£ä¾‹ï¼ˆð‘¦=1ï¼‰ï¼ŒåŽ30%çš„è‚¡ç¥¨ä½œä¸ºè´Ÿä¾‹ï¼ˆð‘¦=âˆ’1ï¼‰ã€‚åˆ©ç”¨xgboostè¿›è¡Œåˆ†ç±»é¢„æµ‹ã€‚ç”±äºŽå¤„ç†æ•°æ®æºä»£ç è¿‡é•¿ï¼Œè¿™é‡Œä¸åšå±•ç¤ºï¼Œä¸”è¯¥éƒ¨åˆ†ä¹Ÿä¸å…·å¤‡å‚è€ƒä»·å€¼ï¼Œè¯»è€…å¯æ ¹æ®è‡ªå·±å¸¸ç”¨ç‰¹å¾å¤„ç†ä¹ æƒ¯è¿›è¡Œå¤„ç†ã€‚
'''

# In[ ]:

import pandas as pd
import numpy as np
import xgboost as xgb
import matplotlib.pyplot as plt

factors = [b'Beta60', b'OperatingRevenueGrowRate', b'NetProfitGrowRate', b'NetCashFlowGrowRate', b'NetProfitGrowRate5Y', b'TVSTD20',
           b'TVSTD6', b'TVMA20', b'TVMA6', b'BLEV', b'MLEV', b'CashToCurrentLiability', b'CurrentRatio', b'REC', b'DAREC', b'GREC',
           b'DASREV', b'SFY12P', b'LCAP', b'ASSI', b'LFLO', b'TA2EV', b'PEG5Y', b'PE', b'PB', b'PS', b'SalesCostRatio', b'PCF', b'CETOP',
           b'TotalProfitGrowRate', b'CTOP', b'MACD', b'DEA', b'DIFF', b'RSI', b'PSY', b'BIAS10', b'ROE', b'ROA', b'ROA5', b'ROE5',
           b'DEGM', b'GrossIncomeRatio', b'ROECut', b'NIAPCut', b'CurrentAssetsTRate', b'FixedAssetsTRate', b'FCFF', b'FCFE', b'PLRC6',
           b'REVS5', b'REVS10', b'REVS20', b'REVS60', b'HSIGMA', b'HsigmaCNE5', b'ChaikinOscillator', b'ChaikinVolatility', b'Aroon',
           b'DDI', b'MTM', b'MTMMA', b'VOL10', b'VOL20', b'VOL5', b'VOL60', b'RealizedVolatility', b'DASTD', b'DDNSR', b'Hurst']

df = pd.read_csv(u'./raw_data/dataset.csv', dtype={"ticker": np.str, "tradeDate": np.str, "next_month_end": np.str},index_col=0, encoding='GBK')
df.head()


# # **æ¨¡åž‹è®­ç»ƒ**
# -----
# ä¸ºäº†èƒ½è®©æ¨¡åž‹åŠæ—¶æŠ“å–åˆ°å¸‚åœºçš„å˜åŒ–ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†ä¸ƒä¸ªé˜¶æ®µæ»šåŠ¨å›žæµ‹æ–¹æ³•ã€‚æ¨¡åž‹è®­ç»ƒåŒºé—´ä¸º20070101è‡³20171231ï¼ŒæŒ‰å¹´ä»½åˆ†ä¸º7ä¸ªå­åŒºé—´ï¼Œå› æ­¤éœ€è¦å¯¹æ¯ä¸ªå­å›žæµ‹çš„ä¸åŒè®­ç»ƒé›†é‡å¤è®­ç»ƒã€‚
# æ¯æ¬¡è®­ç»ƒå®Œæ¨¡åž‹çš„æµ‹è¯•å¿…é¡»é€‰ç”¨è®­ç»ƒæ ·æœ¬å¤–çš„æ•°æ®ï¼Œä¸€èˆ¬æµ‹è¯•æ ·æœ¬é€‰å–ä¸€å¹´çš„æ•°æ®ï¼Œè®­ç»ƒæ ·æœ¬é€‰å–ä¸è¶…è¿‡6å¹´çš„åŽ†å²æ•°æ®ï¼Œå› ä¸ºä¼˜çŸ¿åªæä¾›07å¹´åŽçš„æ•°æ®ï¼Œæ‰€ä»¥åˆå§‹çš„å‡ ä¸ªæ ·æœ¬ä¸è¶³7å¹´ã€‚æ¯”å¦‚ç¬¬ä¸€é˜¶æ®µé€‰å–07-10çš„æ•°æ®ä¸ºè®­ç»ƒæ ·æœ¬ï¼Œ11å¹´ä¸ºæµ‹è¯•æ ·æœ¬ï¼›ç¬¬äºŒé˜¶æ®µé€‰å–07-11çš„æ•°æ®ä½œä¸ºè®­ç»ƒæ ·æœ¬ï¼Œ12å¹´ä¸ºæµ‹è¯•æ ·æœ¬â€¦ä»¥æ­¤å†…æŽ¨ï¼Œæœ€åŽä¸€ä¸ªé˜¶æ®µä¸º11-16çš„æ•°æ®ä¸ºè®­ç»ƒæ ·æœ¬ï¼Œ17å¹´ä¸ºæµ‹è¯•æ ·æœ¬ã€‚
# å¦å¤–æ¨¡åž‹åœ¨è®­ç»ƒä¸­ï¼Œè¾“å…¥æ•°æ®æŒ‰ç…§90%ä¸Ž10%çš„æ¯”ä¾‹æ‹†æˆè®­ç»ƒé›†ä¸ŽéªŒè¯é›†ã€‚è®­ç»ƒä¸­ï¼Œä¿å­˜åœ¨éªŒè¯é›†ä¸Šæ•ˆæžœæœ€å¥½çš„æ¨¡åž‹ã€‚æ¨¡åž‹ä¸­å‚æ•°å¯è°ƒï¼Œè¯»è€…å¯æ›´æ¢ä¸€äº›å‚æ•°ï¼ŒæŸ¥çœ‹ç»“æžœå˜æ›´ã€‚

# In[ ]:

class BoostModel:
    def __init__(self, max_depth=3, subsample=0.95, num_round=2000, early_stopping_rounds=50):
        self.params = {'max_depth': max_depth, 'eta': 0.1, 'silent': 1, 'alpha': 0.5, 'lambda': 0.5, 'eval_metric':'auc', 'subsample':subsample, 'objective': 'binary:logistic'}
        self.num_round = num_round
        self.early_stopping_rounds = early_stopping_rounds


    def fit(self, train_data, train_label,  val_data, val_label):
        dtrain = xgb.DMatrix(train_data, label=train_label)
        deval = xgb.DMatrix(val_data, label=val_label)

        boost_model = xgb.train(self.params, dtrain, num_boost_round=self.num_round, evals=[(dtrain,'train'), (deval, 'eval')], early_stopping_rounds=self.early_stopping_rounds, verbose_eval=False)
        print('get best eval auc : %s, in step %s'%(boost_model.best_score, boost_model.best_iteration))
        self.boost_model = boost_model
        
        return boost_model


    def predict(self, test_data):
        dtest = xgb.DMatrix(test_data)
        predict_score = self.boost_model.predict(dtest, ntree_limit=self.boost_model.best_ntree_limit)
        
        return predict_score
        


# In[ ]:

def get_train_val_test_data(year, split_pct=0.9):
    back_year = max(2007, year-6)
    train_val_df = df[(df['year']>=back_year) & (df['year']<year)]
    train_val_df = train_val_df.sample(frac=1).reset_index(drop=True)
    
    #æ‹†åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†
    train_df = train_val_df.iloc[0:int(len(train_val_df) * split_pct)]
    val_df = train_val_df.iloc[int(len(train_val_df) * split_pct):]
    
    test_df = df[df['year']==year]
    
    return train_df, val_df, test_df

def format_feature_label(origin_df, is_filter=True):
    
    if is_filter:
        origin_df = origin_df[origin_df['label']!=0]
        #å› å­xgboostçš„labelè¾“å…¥èŒƒå›´åªèƒ½æ˜¯[0, 1]ï¼Œéœ€è¦å¯¹åŽŸå§‹labelè¿›è¡Œæ›¿æ¢
        origin_df['label'] = origin_df['label'].replace(-1, 0)
        
    feature = np.array(origin_df[factors])
    label = np.array(origin_df['label'])

    return feature, label

def write_factor_to_csv(df, predict_score, year):
    #è®°å½•æ¨¡åž‹é¢„æµ‹åˆ†æ•°ä¸ºå› å­å€¼ï¼Œè¾“å‡º
    df['factor'] = predict_score
    df = df.loc[:, ['ticker', 'tradeDate', 'label', 'factor']]
    is_header = True
    if year != 2011:
        is_header = False
    
    df.to_csv('./raw_data/factor.csv', mode='a+', encoding='utf-8', header=is_header)

def pipeline():
    boost_model_list = []
    for year in range(2011, 2018):
        print('training model for %s' % year)
        train_df, val_df, test_df = get_train_val_test_data(year)
        boost_model = BoostModel()
        train_feature, train_label = format_feature_label(train_df)
        val_feature, val_label = format_feature_label(val_df)
        
        boost_model.fit(train_feature, train_label, val_feature, val_label)
        
        test_feature, test_label = format_feature_label(test_df, False)
        predict_score = boost_model.predict(test_feature)
        
        write_factor_to_csv(test_df, predict_score, year)
        boost_model_list.append(boost_model)
    
    return boost_model_list

boost_model_list = pipeline()


# # **æ¨¡åž‹ç»“æžœåˆ†æž**
# ------
# ä¸Šè¿°ä¿¡æ¯åªå±•ç¤ºäº†æ¨¡åž‹éªŒè¯é›†ä¸Šçš„æ•ˆæžœï¼ŒçŽ°åœ¨è®©æˆ‘ä»¬æ¥æŸ¥çœ‹ä¸€ä¸‹æ ·æœ¬å¤–çš„å‡†ç¡®çŽ‡å¦‚ä½•ã€‚å¯ä»¥çœ‹åˆ°7ä¸ªé˜¶æ®µçš„å¹³å‡å‡†ç¡®çŽ‡åœ¨57%å·¦å³ï¼Œè¯„ä»·AUCåœ¨60%å·¦å³ã€‚

# In[ ]:

from datetime import datetime
from sklearn.metrics import roc_auc_score

#è®¡ç®—äºŒåˆ†ç±»æ¨¡åž‹æ ·æœ¬å¤–çš„ACCä¸ŽAUC
def get_test_auc_acc():
    df = pd.read_csv('./raw_data/factor.csv')
    #åªæŸ¥çœ‹åŽŸæœ‰labelä¸º+1, -1çš„æ•°æ®
    df = df[df['label'] != 0]
    df.loc[:, 'predict'] = df.loc[:, 'factor'].apply(lambda x : 1 if x > 0.5 else -1)

    acc_list = []
    auc_list = []
    for date, group in df.groupby('tradeDate'):
        df_correct = group[group['predict'] == group['label']]
        correct = len(df_correct) * 1.0 / len(group)
        auc =  roc_auc_score(np.array(group['label']), np.array(group['factor']))
        acc_list.append([date, correct])
        auc_list.append([date, auc])
        
    acc_list = sorted(acc_list, key=lambda x: x[0], reverse=False)
    mean_acc = sum([item[1] for item in acc_list]) / len(acc_list)
    
    auc_list = sorted(auc_list, key=lambda x: x[0], reverse=False)
    mean_auc = sum([item[1] for item in auc_list]) / len(auc_list)
    
    return acc_list, auc_list, round(mean_acc, 2), round(mean_auc, 2)

def plot_accuracy_curve():
    acc_list, auc_list, mean_acc, mean_auc = get_test_auc_acc()

    plt.plot([datetime.strptime(str(item[0]), '%Y%m%d') for item in acc_list], [item[1] for item in acc_list], '-bo')
    plt.plot([datetime.strptime(str(item[0]), '%Y%m%d') for item in auc_list], [item[1] for item in auc_list], '-ro')

    plt.legend([u"acc curve: mean_acc:%s"%mean_acc, u"auc curve: mean auc:%s"%mean_auc], loc='upper left', handlelength=2, handletextpad=0.5, borderpad=0.1)
    plt.ylim((0.3, 0.8))
    plt.show()

plot_accuracy_curve()


# åŒæ—¶ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æŸ¥çœ‹ç‰¹å¾çš„é‡è¦æ€§ï¼Œå¯ä»¥ç›´è§‚æ„Ÿå—å“ªäº›å› å­çš„å½±å“æ¯”è¾ƒé‡è¦ã€‚é¦–å…ˆç»Ÿè®¡å„ä¸ªå¹´ä»½çš„ç‰¹å¾æŽ’åºï¼ŒæŽ’åºèŒƒå›´1~70ï¼Œ1ä¸ºå½±å“æœ€å¼±ï¼Œ70ä¸ºå½±å“æœ€å¼ºã€‚æœ€åŽæ±‚å‡ºå‡å€¼ã€‚
# ä»¥ä¸‹è¡¨æ ¼åˆ—å‡ºäº†æŽ’åå‰10å’ŒæŽ’ååŽ10çš„å› å­åç§°ï¼ŒæŸ¥çœ‹ä¼˜çŸ¿å› å­æ–‡æ¡£ï¼ŒçŸ¥é“Aroon(åŠ¨é‡å› å­)ï¼ŒHurst(èµ«æ–¯ç‰¹æŒ‡æ•°, æŠ€æœ¯æŒ‡æ ‡ç±»å› å­), ChaikinOscillator(ä½³åº†æŒ‡æ ‡, æŠ€æœ¯æŒ‡æ ‡ç±»å› å­), LCAP(å¯¹æ•°å¸‚å€¼), REVS20(åŠ¨é‡ç±»å› å­)åœ¨æ¨¡åž‹åˆ†ç±»ä¸­è¡¨çŽ°æœ€ä¸ºé‡è¦ã€‚è¿™é‡Œéœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¾ˆå¤šå› å­ä¹‹é—´æœ¬èº«æœ‰ç›¸å…³æ€§ï¼Œæ‰€ä»¥è¡¨çŽ°å¥½åå¹¶ä¸èƒ½ä½œä¸ºè¯¥å› å­çš„å”¯ä¸€åˆ¤æ–­æ ‡å‡†ã€‚ 

# In[ ]:

def get_feature_importance():
    df = pd.DataFrame(index=factors, columns=range(2011, 2018))
    for i, column in enumerate(range(2011, 2018)):
        feature_importance = boost_model_list[i].boost_model.get_score(importance_type='weight')
        df[column] = pd.Series(index=[factors[int(key.replace('f', ''))] for key, value in feature_importance.items()], data =[value for key, value in feature_importance.items()])
        df[column] = df[column].fillna(0.0)
        df[column] = 1 + np.argsort(np.argsort(df[column]))
        
    df['all'] = df.mean(axis=1)
    
    return df.sort_values('all', ascending=False)
        
feature_importance_df = get_feature_importance()
feature_importance_df.iloc[np.r_[0:10, -10:0]]


# # **å› å­å›žæµ‹**
# ä¸Šè¿°æ¨¡åž‹åœ¨æµ‹è¯•çš„æ—¶å€™ï¼Œä¸ºæ¯ä¸ªè‚¡ç¥¨æ‰“äº†ä¸€ä¸ªåˆ†æ•°ï¼Œå¯ä»¥è®¤ä¸ºæ˜¯å°†è¾“å…¥çš„70ä¸ªå› å­è½¬æ¢æˆäº†ä¸€ä¸ªé›†æˆçš„â€œå› å­â€ï¼ŒçŽ°åœ¨è®©æˆ‘ä»¬æ¥æ£€æµ‹è¿™ä¸ªéžçº¿æ€§è¾“å‡ºçš„å› å­å›žæµ‹æ•ˆæžœå¦‚ä½•ã€‚
# æœ¬æ–‡é€‰å–äº†ä¸­è¯500ä½œä¸ºåŸºå‡†ï¼Œå¹¶åˆ†ä¸º5ç»„æŸ¥çœ‹æ¯ç±»æ•ˆæžœã€‚å›žæµ‹æ¡†æž¶å‚è€ƒä¼˜çŸ¿[APIæ–‡æ¡£](https://uqer.io/help/api/#quick_backtest%E5%8F%82%E6%95%B0%E4%BC%98%E5%8C%96) ä¸­quick_backtestå‚æ•°ä¼˜åŒ–ï¼Œç”»å›¾å‚è€ƒä¹‹å‰ç¤¾åŒºè´´[å‡¤é¸£æœé˜³ - è‚¡ä»·æ—¥å†…æ¨¡å¼åˆ†æž](https://uqer.io/community/share/581aa67a228e5b43fa5c363f)

# In[ ]:

signal_df = pd.read_csv(u'./raw_data/factor.csv', dtype={"ticker": np.str, "tradeDate": np.str},index_col=0, encoding='GBK')
signal_df['ticker'] = signal_df['ticker'].apply(lambda x: x+'.XSHG' if x[:2] in ['60'] else x+'.XSHE')
signal_df = signal_df[[u'ticker', u'tradeDate', u'factor']]

# ç”³ä¸‡è¡Œä¸š
sw_frame = DataAPI.EquIndustryGet(industryVersionCD=u"010303",industry=u"",secID=u"",ticker=u"",intoDate=u"",field=u"",pandas="1")
sw_frame = sw_frame[sw_frame.isNew==1][['secID', 'ticker', 'industryName1', 'industryName2', 'industryName3']]
sw_frame = sw_frame[['secID', 'industryName1']]
sw_frame.columns = ['ticker', 'industryName1']
signal_df = signal_df.merge(sw_frame, on=['ticker'], how='left')
signal_df.head()


# In[ ]:

from CAL.PyCAL import * 

# -----------å›žæµ‹å‚æ•°éƒ¨åˆ†å¼€å§‹ï¼Œå¯ç¼–è¾‘------------
start = '2011-01-01'                       # å›žæµ‹èµ·å§‹æ—¶é—´
end = '2017-12-31'                         # å›žæµ‹ç»“æŸæ—¶é—´
benchmark = 'ZZ500'                        # ç­–ç•¥å‚è€ƒæ ‡å‡†
universe = DynamicUniverse('ZZ500')           # è¯åˆ¸æ± ï¼Œæ”¯æŒè‚¡ç¥¨å’ŒåŸºé‡‘
capital_base = 10000000                     # èµ·å§‹èµ„é‡‘
freq = 'd'                              
refresh_rate = Monthly(1)  

factor_data = signal_df[['ticker', 'tradeDate', 'factor']]     # è¯»å–å› å­æ•°æ®
factor_data = factor_data.set_index('tradeDate', drop=True)
q_dates = factor_data.index.values

accounts = {
    'fantasy_account': AccountConfig(account_type='security', capital_base=10000000)
}

# ---------------å›žæµ‹å‚æ•°éƒ¨åˆ†ç»“æŸ----------------

# æŠŠå›žæµ‹å‚æ•°å°è£…åˆ° SimulationParameters ä¸­ï¼Œä¾› quick_backtest ä½¿ç”¨
sim_params = quartz.SimulationParameters(start, end, benchmark, universe, capital_base, refresh_rate=refresh_rate, accounts=accounts)
# èŽ·å–å›žæµ‹è¡Œæƒ…æ•°æ®
data = quartz.get_backtest_data(sim_params)
# è¿è¡Œç»“æžœ
results = {}

# è°ƒæ•´å‚æ•°(é€‰å–è‚¡ç¥¨çš„é›†æˆå› å­äº”åˆ†ä½æ•°)ï¼Œè¿›è¡Œå¿«é€Ÿå›žæµ‹
for quantile_five in range(1, 6):
    
    # ---------------ç­–ç•¥é€»è¾‘éƒ¨åˆ†----------------
    
    def initialize(context):                   # åˆå§‹åŒ–è™šæ‹Ÿè´¦æˆ·çŠ¶æ€
        pass

    def handle_data(context): 
        account = context.get_account('fantasy_account')
        current_universe = context.get_universe('stock', exclude_halt=True)
        pre_date = context.previous_date.strftime("%Y%m%d")
        if pre_date not in q_dates:            
            return

        # æ‹¿å–è°ƒä»“æ—¥å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„å› å­ï¼Œå¹¶æŒ‰ç…§ç›¸åº”ååˆ†ä½é€‰æ‹©è‚¡ç¥¨
        q = factor_data.ix[pre_date].dropna()
        q = q.set_index('ticker', drop=True)
        q = q.ix[current_universe]
        
        q_min = q['factor'].quantile((quantile_five-1)*0.2)
        q_max = q['factor'].quantile(quantile_five*0.2)
        my_univ = q[(q['factor']>=q_min) & (q['factor']<q_max)].index.values

       # äº¤æ˜“éƒ¨åˆ†
        positions = account.get_positions()
        sell_list = [stk for stk in positions if stk not in my_univ]
        for stk in sell_list:
            order_to(stk,0)
        
        # åœ¨ç›®æ ‡è‚¡ç¥¨æ± ä¸­çš„ï¼Œç­‰æƒä¹°å…¥
        for stk in my_univ:
            order_pct_to(stk, 1.0/len(my_univ))


    # ç”Ÿæˆç­–ç•¥å¯¹è±¡
    strategy = quartz.TradingStrategy(initialize, handle_data)
    # ---------------ç­–ç•¥å®šä¹‰ç»“æŸ----------------
    
    # å¼€å§‹å›žæµ‹
    bt, perf = quartz.quick_backtest(sim_params, strategy, data=data)

    # ä¿å­˜è¿è¡Œç»“æžœï¼Œ1ä¸ºå› å­æœ€å¼ºç»„ï¼Œ5ä¸ºå› å­æœ€å¼±ç»„
    results[6 - quantile_five] = {'max_drawdown': perf['max_drawdown'], 'sharpe': perf['sharpe'], 'alpha': perf['alpha'], 'beta': perf['beta'],
                              'information_ratio': perf['information_ratio'], 'annualized_return': perf['annualized_return'], 'bt': bt}    

    print str(quantile_five),
print 'done'


# In[ ]:

import seaborn as sns
sns.set_style('white')

fig = plt.figure(figsize=(10,8))
fig.set_tight_layout(True)
ax1 = fig.add_subplot(211)
ax2 = fig.add_subplot(212)
ax1.grid()
ax2.grid()

for qt in results:
    bt = results[qt]['bt']

    data = bt[[u'tradeDate',u'portfolio_value',u'benchmark_return']]
    data['portfolio_return'] = data.portfolio_value/data.portfolio_value.shift(1) - 1.0   # æ€»å¤´å¯¸æ¯æ—¥å›žæŠ¥çŽ‡
    data['portfolio_return'].ix[0] = data['portfolio_value'].ix[0]/	10000000.0 - 1.0
    data['excess_return'] = data.portfolio_return - data.benchmark_return                 # æ€»å¤´å¯¸æ¯æ—¥è¶…é¢å›žæŠ¥çŽ‡
    data['excess'] = data.excess_return + 1.0
    data['excess'] = data.excess.cumprod()                # æ€»å¤´å¯¸å¯¹å†²æŒ‡æ•°åŽçš„å‡€å€¼åºåˆ—
    data['portfolio'] = data.portfolio_return + 1.0     
    data['portfolio'] = data.portfolio.cumprod()          # æ€»å¤´å¯¸ä¸å¯¹å†²æ—¶çš„å‡€å€¼åºåˆ—
    data['benchmark'] = data.benchmark_return + 1.0
    data['benchmark'] = data.benchmark.cumprod()          # benchmarkçš„å‡€å€¼åºåˆ—
    results[qt]['hedged_max_drawdown'] = max([1 - v/max(1, max(data['excess'][:i+1])) for i,v in enumerate(data['excess'])])  # å¯¹å†²åŽå‡€å€¼æœ€å¤§å›žæ’¤
    results[qt]['hedged_volatility'] = np.std(data['excess_return'])*np.sqrt(252)
    results[qt]['hedged_annualized_return'] = (data['excess'].values[-1])**(252.0/len(data['excess'])) - 1.0
    ax1.plot(data['tradeDate'], data[['portfolio']], label=str(qt))
    ax2.plot(data['tradeDate'], data[['excess']], label=str(qt))
    

ax1.legend(loc=0)
ax2.legend(loc=0)
ax1.set_ylabel(u"å‡€å€¼", fontproperties=font, fontsize=16)
ax2.set_ylabel(u"å¯¹å†²å‡€å€¼", fontproperties=font, fontsize=16)
ax1.set_title(u"å› å­ä¸åŒäº”åˆ†ä½æ•°åˆ†ç»„é€‰è‚¡å‡€å€¼èµ°åŠ¿", fontproperties=font, fontsize=16)
ax2.set_title(u"å› å­ä¸åŒäº”åˆ†ä½æ•°åˆ†ç»„é€‰è‚¡å¯¹å†²ä¸­è¯500æŒ‡æ•°åŽå‡€å€¼èµ°åŠ¿", fontproperties=font, fontsize=16)

# results è½¬æ¢ä¸º DataFrame
results_pd = pd.DataFrame(results).T.sort_index()

results_pd = results_pd[[u'alpha', u'beta', u'information_ratio', u'sharpe', u'annualized_return', u'max_drawdown',  
                         u'hedged_annualized_return', u'hedged_max_drawdown', u'hedged_volatility']]

cols = [(u'é£Žé™©æŒ‡æ ‡', u'Alpha'), (u'é£Žé™©æŒ‡æ ‡', u'Beta'), (u'é£Žé™©æŒ‡æ ‡', u'ä¿¡æ¯æ¯”çŽ‡'), (u'é£Žé™©æŒ‡æ ‡', u'å¤æ™®æ¯”çŽ‡'), (u'çº¯è‚¡ç¥¨å¤šå¤´æ—¶', u'å¹´åŒ–æ”¶ç›Š'),
        (u'çº¯è‚¡ç¥¨å¤šå¤´æ—¶', u'æœ€å¤§å›žæ’¤'), (u'å¯¹å†²åŽ', u'å¹´åŒ–æ”¶ç›Š'), (u'å¯¹å†²åŽ', u'æœ€å¤§å›žæ’¤'), (u'å¯¹å†²åŽ', u'æ”¶ç›Šæ³¢åŠ¨çŽ‡')]
results_pd.columns = pd.MultiIndex.from_tuples(cols)
results_pd.index.name = u'äº”åˆ†ä½ç»„åˆ«'
results_pd


# å¯ä»¥ä»Žä¸Šå›¾çœ‹åˆ°1-5ç»„æœ‰æ˜Žæ˜¾å•è°ƒå…³ç³»ï¼Œå¯ä»¥è¯´æ˜Žè¯¥å› å­æœ‰ä¸€å®šçš„é€‰è‚¡èƒ½åŠ›ã€‚
# ä¸Šè¿°ä¸€äº›è¾“å…¥ä»…ä½œå‚è€ƒï¼Œè¯»è€…å¯ä»¥æ ¹æ®è‡ªå·±éœ€è¦æ›´æ”¹æ¨¡åž‹è¾“å…¥åŸºç¡€å› å­ï¼Œæ¨¡åž‹è®­ç»ƒå‚æ•°åŠæ¡†æž¶ï¼Œæ¥è¿›è¡Œå› å­å›žæµ‹ã€‚

# # **å‚è€ƒ**
# 1ã€ [å‡¤é¸£æœé˜³ - è‚¡ä»·æ—¥å†…æ¨¡å¼åˆ†æž](https://uqer.io/community/share/581aa67a228e5b43fa5c363f)
# 2ã€ åŽæ³°è¯åˆ¸ ã€Šäººå·¥æ™ºèƒ½é€‰è‚¡ä¹‹ Boosting æ¨¡åž‹ã€‹
