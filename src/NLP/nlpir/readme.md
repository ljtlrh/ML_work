http://ictclas.nlpir.org/nlpir/
https://github.com/tsroten/pynlpir
NLPIR汉语分词系统,主要功能包括中文分词；词性标注；命名实体识别；用户词典功能；支持GBK编码、UTF8编码、BIG5编码。新增微博分词、新词发现与关键词提取；张华平博士先后倾力打造十余年，内核升级10次。（http://ictclas.nlpir.org/）
哈工大LTP和中科院NLPIR中文分词比较:
https://blog.csdn.net/churximi/article/details/51174317

1.LTP有自动分句功能，NLPIR没有分句功能；

LTP的分句是根据中文标点里的句号、问号、感叹号、分号、省略号。


2.NLPIR分词有两种方式，分词结果完全一致。
（1）利用分词函数NLPIR_ParagraphProcess；
（2）利用NLPIR_FileProcess对整个文本进行分词。
其中第（2）种有时候会出现bug，分词进行到文本某一处时会进行不下去。


3.LTP有在线API和本地两种：
（1）在线API受限于URI构造规则，不能提交一些特殊字符（如#、&、+、换行符、英文分号等），而且存在极少数未知的组合bug（比如and和in居然不能同时提交）；
（2）本地LTP在分词时则不受上述限制。


符号
问题
换行符
程序直接报错
&
会影响句子处理（句子会被截断）
英文分号;
会影响句子处理（句子会被截断）
#
会影响句子处理（程序直接报错，如“C#”）
+
会被系统删除（如“互联网+”、“C++”）

        基于上述原因，调用在线API分词和本地分词两种方式的分词结果会有差异，而且除了上述特殊字符引起的差异外，还会有其他一些极少数差异。



差异统计：3000个句子，有40句存在分词差异。
“+”引起：21句
“#”引起：2句
其他：17句


LTP本地
LTP在线
 
LTP本地
LTP在线
异常 点
异常点
 
多 年
多年
不好的
不好 的
 
过 关
过关
书 挺 不错 的
书挺 不错 的
 
白 学 了
白学 了
很快
很 快
 
就 此
就此
赶上
赶 上
 
网络类 的 图书
网络 类 的 图书
初学者
初 学者
 
缺 失
缺失

上述例子并不绝对，在有些句子里，在线切分也能够将“初学者”切分出来，而本地切分却会将其切分成“初”“学者”。



4.用户词典：
（1）NLPIR分词时将优先使用用户词典；
（2）LTP用户词典：官方补充说“LTP的分词模块并非采用词典匹配的策略，外部词典以特征方式加入机器学习算法，并不能保证所有的词都是按照词典里的方式进行切分”；
所以LTP有些词加入用户词典也无效，比如“C++”、“C#”、“互联网+”。


5.哈工大LTP与中科院NLPIR分词差异

        试验了3000句，去除哈工大停用词后有31259个词，其中1505句分词结果完全一样。

        注意：列表只是给出一些词在某些句子里的切分情况，并不表示在所有句子里都是这样切分。在有些情况下，哈工大和中科院的切分方法甚至会反过来。

        整体上感觉中科院NLPIR在很多词上更倾向于分得更小。